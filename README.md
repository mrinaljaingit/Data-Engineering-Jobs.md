### üìã Data Engineering Jobs Summary


| S.No | Company Name       | Top 5 Required Skills                                      | Experience (Years) | Responsibilities Summary                                                                 | Location             | Estimated Pay Range (INR) |
|------|--------------------|------------------------------------------------------------|--------------------|------------------------------------------------------------------------------------------|----------------------|---------------------------|
| 1    | Deloitte           | SQL, Data Warehousing, Basel III, Data Migration, Profiling | 4+                 | - Support Basel III data migration<br>- Validate and reconcile risk data<br>- Perform profiling<br>- Collaborate with SMEs<br>- Prepare documentation | Chennai              | ‚Çπ12‚Äì14 LPA                |
| 2    | United Airlines    | SQL (Teradata), T-SQL, Tableau, Spotfire, Data Analysis     | 0‚Äì2                | - Extract and analyze operational data<br>- Build dashboards<br>- Present insights<br>- Support leadership reporting | Gurugram             | ‚Çπ8‚Äì10 LPA                 |
| 3    | Lowe‚Äôs India       | Excel, SQL, Python, Tableau, Media Insights                 | 1‚Äì3                | - Analyze campaign data<br>- Generate client reports<br>- Conduct A/B testing<br>- Present insights | Bengaluru            | ‚Çπ10‚Äì12 LPA                |
| 4    | Oracle             | Snowflake, SQL, AWS, CI/CD, GoldenGate                     | 4+                 | - Design Snowflake models<br>- Real-time ingestion<br>- Cloud integration<br>- Optimize performance<br>- Document architecture | Bengaluru            | ‚Çπ18‚Äì22 LPA                |
| 5    | Salesforce         | Spark, Kafka, DBT, AWS, Trino                              | 5+                 | - Build batch/streaming pipelines<br>- Design semantic layers<br>- Apply CI/CD<br>- Collaborate cross-org<br>- Support AI agents | San Francisco (Remote) | ‚Çπ35‚Äì40 LPA (‚Çπ42‚Äì48 LPA globally) |
| 6    | DataKryptonAI      | PySpark, Python, Databricks, CI/CD, Airflow                | 3+                 | - Build ETL pipelines<br>- Model healthcare data<br>- Ensure data quality<br>- Collaborate with analysts<br>- Automate deployments | Remote (India)       | ‚Çπ14‚Äì16 LPA                |
| 7    | BlackRock          | Python, Airflow, DBT, Snowflake, Kubernetes                | 5‚Äì8                | - Build containerized pipelines<br>- Validate data quality<br>- Optimize workflows<br>- Develop APIs<br>- Collaborate with teams | Bengaluru            | ‚Çπ22‚Äì26 LPA                |
| 8    | Accenture          | Data Engineering, BigQuery, GCP, PLSQL, ETL                | 5+                 | - Design data solutions<br>- Implement ETL<br>- Ensure data quality<br>- Mentor juniors<br>- Collaborate across teams | Bengaluru            | ‚Çπ16‚Äì18 LPA                |
| 9    | Lancesoft ME       | PySpark, Airflow, CDP, DevOps, Python                      | 4+                 | - Build scalable pipelines<br>- Optimize ETL workflows<br>- Ensure governance<br>- Support AI/ML use cases<br>- Automate deployments | Remote               | ‚Çπ14‚Äì16 LPA                |
| 10   | Tredence Inc.      | PySpark, SQL, GCP, Data Modeling, Git                      | 1‚Äì2                | - Develop pipelines on GCP<br>- Write efficient SQL<br>- Apply warehouse principles<br>- Collaborate with clients<br>- Support agile delivery | Multiple (India)     | ‚Çπ10‚Äì12 LPA                |
| 11   | Lancesoft Middle East | PySpark, Airflow, CDP, DevOps, Python                  | 4+                 | - Build scalable pipelines<br>- Optimize ETL workflows<br>- Ensure governance<br>- Support AI/ML use cases<br>- Automate deployments | Remote               | ‚Çπ14‚Äì16 LPA                |


---

# üîù Top 5 Tools to Learn (Ranked)

| Rank | Tool / Technology | Why It‚Äôs Important |
|------|-------------------|--------------------|
| 1Ô∏è‚É£   | **SQL**            | Core for querying, transforming, and validating data across all roles. |
| 2Ô∏è‚É£   | **Python**         | Widely used for ETL, data manipulation, and integration with big data tools. |
| 3Ô∏è‚É£   | **PySpark / Spark**| Essential for distributed data processing and scalable pipelines. |
| 4Ô∏è‚É£   | **Airflow**        | Popular orchestration tool for managing workflows and scheduling jobs. |
| 5Ô∏è‚É£   | **Databricks**     | Cloud-native platform for big data analytics, Spark jobs, and ML integration. |

---

# üè¢ Expectations Across Companies

### 1Ô∏è‚É£ SQL  (**Used by:** Deloitte, United Airlines, Oracle, BlackRock, Tredence, Accenture)
**Expectations:**
- Write complex queries (joins, window functions, aggregations)
- Optimize performance and validate data quality
- Translate business logic into SQL transformations  
**Focus Areas:**
- Query optimization
- Data modeling (star/snowflake schemas)
- Integration with BI tools and ETL pipelines

---

### 2Ô∏è‚É£ Python  (**Used by:** DataKryptonAI, BlackRock, Lancesoft, Tredence, Oracle)  
**Expectations:**
- Build ETL/ELT pipelines
- Automate data workflows and CI/CD
- Collaborate with analysts and data scientists  
**Focus Areas:**
- pandas, PySpark, DBT integration
- Writing modular, testable code
- Performance tuning and error handling

---

### 3Ô∏è‚É£ PySpark / Spark  (**Used by:** Salesforce, DataKryptonAI, Tredence, Lancesoft)
**Expectations:**
- Handle large-scale data transformations
- Build batch and streaming pipelines
- Optimize Spark jobs for performance  
**Focus Areas:**
- RDDs vs DataFrames
- Caching, partitioning, and AQE
- Integration with cloud platforms (Databricks, EMR)

---

### 4Ô∏è‚É£ Airflow  (**Used by:** BlackRock, Lancesoft, DataKryptonAI)
**Expectations:**
- Schedule and monitor data workflows
- Handle dependencies and retries
- Integrate with cloud and CI/CD tools  
**Focus Areas:**
- DAG creation and parameterization
- Sensor and operator customization
- Logging and alerting mechanisms

---

### 5Ô∏è‚É£ Databricks  (**Used by:** DataKryptonAI, Oracle (indirectly), Salesforce)
**Expectations:**
- Run Spark jobs and notebooks
- Use Delta Lake for data reliability
- Collaborate across teams using shared workspaces  
**Focus Areas:**
- Cluster configuration and job scheduling
- Delta Lake features (time travel, schema enforcement)
- ML integration and notebook automation

---

## üéØ Learning Focus Areas

### üîß Technical Skills
- Master SQL with real-world datasets (joins, aggregations, window functions)
- Build ETL pipelines using Python and PySpark
- Practice orchestration with Airflow (build DAGs, monitor jobs)
- Explore Databricks notebooks and Delta Lake features
- Learn data modeling: star/snowflake schemas, SCDs, CDC

### üìä Business & Communication
- Translate business requirements into technical specs
- Document data lineage and transformation logic
- Present insights using Tableau/Power BI
- Collaborate with cross-functional teams (analysts, product managers)

### ‚òÅÔ∏è Cloud & DevOps
- Get hands-on with GCP, AWS, or Azure (especially BigQuery, S3, Lambda)
- Learn CI/CD basics with GitHub Actions or Jenkins
- Understand data governance and quality tools (Great Expectations, Soda)

---

*Prepared for structured learning and interview readiness in data engineering.*
