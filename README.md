### üìã Data Engineering Jobs Summary


| S.No | Company Name       | Top 5 Required Skills                                      | Experience (Years) | Responsibilities Summary                                                                 | Location             | Estimated Pay Range (INR) |
|------|--------------------|------------------------------------------------------------|--------------------|------------------------------------------------------------------------------------------|----------------------|---------------------------|
| 1    | Deloitte           | SQL, Data Warehousing, Basel III, Data Migration, Profiling | 4+                 | - Support Basel III data migration<br>- Validate and reconcile risk data<br>- Perform profiling<br>- Collaborate with SMEs<br>- Prepare documentation | Chennai              | ‚Çπ12‚Äì14 LPA                |
| 2    | United Airlines    | SQL (Teradata), T-SQL, Tableau, Spotfire, Data Analysis     | 0‚Äì2                | - Extract and analyze operational data<br>- Build dashboards<br>- Present insights<br>- Support leadership reporting | Gurugram             | ‚Çπ8‚Äì10 LPA                 |
| 3    | Lowe‚Äôs India       | Excel, SQL, Python, Tableau, Media Insights                 | 1‚Äì3                | - Analyze campaign data<br>- Generate client reports<br>- Conduct A/B testing<br>- Present insights | Bengaluru            | ‚Çπ10‚Äì12 LPA                |
| 4    | Oracle             | Snowflake, SQL, AWS, CI/CD, GoldenGate                     | 4+                 | - Design Snowflake models<br>- Real-time ingestion<br>- Cloud integration<br>- Optimize performance<br>- Document architecture | Bengaluru            | ‚Çπ18‚Äì22 LPA                |
| 5    | Salesforce         | Spark, Kafka, DBT, AWS, Trino                              | 5+                 | - Build batch/streaming pipelines<br>- Design semantic layers<br>- Apply CI/CD<br>- Collaborate cross-org<br>- Support AI agents | San Francisco (Remote) | ‚Çπ35‚Äì40 LPA (‚Çπ42‚Äì48 LPA globally) |
| 6    | DataKryptonAI      | PySpark, Python, Databricks, CI/CD, Airflow                | 3+                 | - Build ETL pipelines<br>- Model healthcare data<br>- Ensure data quality<br>- Collaborate with analysts<br>- Automate deployments | Remote (India)       | ‚Çπ14‚Äì16 LPA                |
| 7    | BlackRock          | Python, Airflow, DBT, Snowflake, Kubernetes                | 5‚Äì8                | - Build containerized pipelines<br>- Validate data quality<br>- Optimize workflows<br>- Develop APIs<br>- Collaborate with teams | Bengaluru            | ‚Çπ22‚Äì26 LPA                |
| 8    | Accenture          | Data Engineering, BigQuery, GCP, PLSQL, ETL                | 5+                 | - Design data solutions<br>- Implement ETL<br>- Ensure data quality<br>- Mentor juniors<br>- Collaborate across teams | Bengaluru            | ‚Çπ16‚Äì18 LPA                |
| 9    | Lancesoft ME       | PySpark, Airflow, CDP, DevOps, Python                      | 4+                 | - Build scalable pipelines<br>- Optimize ETL workflows<br>- Ensure governance<br>- Support AI/ML use cases<br>- Automate deployments | Remote               | ‚Çπ14‚Äì16 LPA                |
| 10   | Tredence Inc.      | PySpark, SQL, GCP, Data Modeling, Git                      | 1‚Äì2                | - Develop pipelines on GCP<br>- Write efficient SQL<br>- Apply warehouse principles<br>- Collaborate with clients<br>- Support agile delivery | Multiple (India)     | ‚Çπ10‚Äì12 LPA                |
| 11   | Lancesoft Middle East | PySpark, Airflow, CDP, DevOps, Python                  | 4+                 | - Build scalable pipelines<br>- Optimize ETL workflows<br>- Ensure governance<br>- Support AI/ML use cases<br>- Automate deployments | Remote               | ‚Çπ14‚Äì16 LPA                |


---

# üîù Top 5 Tools to Learn (Ranked)

| Rank | Tool / Technology | Why It‚Äôs Important |
|------|-------------------|--------------------|
| 1Ô∏è‚É£   | **SQL**            | Core for querying, transforming, and validating data across all roles. |
| 2Ô∏è‚É£   | **Python**         | Widely used for ETL, data manipulation, and integration with big data tools. |
| 3Ô∏è‚É£   | **PySpark / Spark**| Essential for distributed data processing and scalable pipelines. |
| 4Ô∏è‚É£   | **Airflow**        | Popular orchestration tool for managing workflows and scheduling jobs. |
| 5Ô∏è‚É£   | **Databricks**     | Cloud-native platform for big data analytics, Spark jobs, and ML integration. |

---

# üè¢ Expectations Across Companies

### 1Ô∏è‚É£ SQL  (**Used by:** Deloitte, United Airlines, Oracle, BlackRock, Tredence, Accenture)
**Expectations:**
- Write complex queries (joins, window functions, aggregations)
- Optimize performance and validate data quality
- Translate business logic into SQL transformations  
**Focus Areas:**
- Query optimization
- Data modeling (star/snowflake schemas)
- Integration with BI tools and ETL pipelines

---

### 2Ô∏è‚É£ Python  (**Used by:** DataKryptonAI, BlackRock, Lancesoft, Tredence, Oracle)  
**Expectations:**
- Build ETL/ELT pipelines
- Automate data workflows and CI/CD
- Collaborate with analysts and data scientists  
**Focus Areas:**
- pandas, PySpark, DBT integration
- Writing modular, testable code
- Performance tuning and error handling

---

### 3Ô∏è‚É£ PySpark / Spark  (**Used by:** Salesforce, DataKryptonAI, Tredence, Lancesoft)
**Expectations:**
- Handle large-scale data transformations
- Build batch and streaming pipelines
- Optimize Spark jobs for performance  
**Focus Areas:**
- RDDs vs DataFrames
- Caching, partitioning, and AQE
- Integration with cloud platforms (Databricks, EMR)

---

### 4Ô∏è‚É£ Airflow  (**Used by:** BlackRock, Lancesoft, DataKryptonAI)
**Expectations:**
- Schedule and monitor data workflows
- Handle dependencies and retries
- Integrate with cloud and CI/CD tools  
**Focus Areas:**
- DAG creation and parameterization
- Sensor and operator customization
- Logging and alerting mechanisms

---

### 5Ô∏è‚É£ Databricks  (**Used by:** DataKryptonAI, Oracle (indirectly), Salesforce)
**Expectations:**
- Run Spark jobs and notebooks
- Use Delta Lake for data reliability
- Collaborate across teams using shared workspaces  
**Focus Areas:**
- Cluster configuration and job scheduling
- Delta Lake features (time travel, schema enforcement)
- ML integration and notebook automation

---

## üéØ Learning Focus Areas

### üîß Technical Skills
- Master SQL with real-world datasets (joins, aggregations, window functions)
- Build ETL pipelines using Python and PySpark
- Practice orchestration with Airflow (build DAGs, monitor jobs)
- Explore Databricks notebooks and Delta Lake features
- Learn data modeling: star/snowflake schemas, SCDs, CDC

### üìä Business & Communication
- Translate business requirements into technical specs
- Document data lineage and transformation logic
- Present insights using Tableau/Power BI
- Collaborate with cross-functional teams (analysts, product managers)

### ‚òÅÔ∏è Cloud & DevOps
- Get hands-on with GCP, AWS, or Azure (especially BigQuery, S3, Lambda)
- Learn CI/CD basics with GitHub Actions or Jenkins
- Understand data governance and quality tools (Great Expectations, Soda)

---

*Prepared for structured learning and interview readiness in data engineering.*


# üìë Shortlist of Companies Hiring Data Engineers (3‚Äì5 yrs Exp, ‚Çπ16‚Äì20 LPA)

Below is a curated list of organizations actively recruiting mid-level data engineers in India, offering compensation in‚Äîor overlapping‚Äîthe ‚Çπ16‚Äì20 LPA band.

| Company                 | Role / Team                    | Exp. Range | Salary Range (LPA) | Key Tech Focus                              |
|-------------------------|--------------------------------|------------|--------------------|----------------------------------------------|
| Google                  | Data Engineer ‚Äì Platforms      | 3‚Äì5 yrs    | 15‚Äì40              | SQL, Python, BigQuery, Data Modeling, GCP    |
| Meta (Facebook)         | Data Engineer                  | 3‚Äì5 yrs    | 18‚Äì45              | Spark, Presto, Kafka, DBT, Distributed APIs  |
| Microsoft               | Data Engineer                  | 3‚Äì5 yrs    | 14‚Äì35              | Azure Data Factory, Synapse, SQL, Python     |
| Amazon Dev Centre       | Data Engineer ‚Äì Data Platforms | 3‚Äì5 yrs    | 4.8‚Äì26             | EMR, Redshift, Spark, AWS Glue, Python       |
| IBM India               | Data Engineer ‚Äì Integration    | 2‚Äì5 yrs    | 5.5‚Äì25             | Python, SQL, DataStage, Cloud Pak for Data   |
| Accenture               | Data Engineer                  | 3‚Äì5 yrs    | 8‚Äì22               | SQL, PySpark, Data Factory, GCP/Azure        |
| Xohani Solutions        | Azure Data Engineer            | 4‚Äì5 yrs    | 15‚Äì21              | Azure Data Factory, Kubernetes, SQL, Python  |
| Lorvensoft Technology   | Data Engineer                  | 5 yrs      | 11‚Äì20              | PySpark, Python, SQL, Data Modeling          |
| Risk Resources          | Data Engineer                  | 2‚Äì5 yrs    | 12‚Äì22              | SQL, Python, ETL Frameworks, DBT             |
| Recro                   | Data Engineer                  | 5 yrs      | 20‚Äì25              | SQL, Python, Cloud Pipelines, CI/CD          |

_Sources:_  
‚Äì Mid-level salary benchmarks in India (3‚Äì5 yrs exp): ‚Çπ10‚Äì20 LPA  
‚Äì Google, Microsoft, Amazon, Meta compensation ranges  
‚Äì IBM India & Amazon estimates (AmbitionBox)  
‚Äì Active openings & salary bands (Wellfound)  
‚Äì Naukri top hirers list (Accenture, Oracle, EY, PwC)  

---

## üéØ What to Focus On

- **SQL & Data Modeling**  
  Master complex queries (joins, window functions) and dimensional modeling (star/snowflake).

- **Programming & Frameworks**  
  Solid Python skills (pandas, PySpark) and familiarity with Spark‚Äôs RDD/DataFrame APIs.

- **Cloud Ecosystems**  
  Hands-on with GCP (BigQuery, Dataflow) or AWS/Azure data services (Glue, Data Factory, Synapse).

- **Orchestration & CI/CD**  
  Build and monitor pipelines using Airflow/Data Factory; implement automated tests and deployment.

- **Streaming & Real-Time**  
  Exposure to Kafka/Flink for event-driven processing, and understanding partitioning/rebalance strategies.

- **Documentation & Collaboration**  
  Translate business requirements to BRDs/FSDs; maintain clear data lineage and work cross-functionally.

Tailor your resume and interview prep around these areas to align with what these top employers seek. Good luck!  

